#!/usr/bin/env python

import numpy
from .GDM_Imagine_Dimensional.episodic_gwr import EpisodicGWR
from .GDM_Imagine_Dimensional import gtls
from .FaceChannelUtils import modelDictionary, modelLoader

import numpy as np

np.seterr(divide='ignore', invalid='ignore')

import re
import os

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}

import tensorflow as tf

tf.get_logger().setLevel('ERROR')

from PIL import Image as PILImage

import shutil
import cv2
import os
import sys

parentDir = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
sys.path.append(parentDir)

# input size for the dimensional model
faceSize = (96, 96)

frameSize = (480, 640)

trained_GWRs = (None, None)

CAAE_LoadPath = "./EmotionCapturing/FaceChannel/CLModel/checkpoint/"


def sort_nicely(l):
    convert = lambda text: int(text) if text.isdigit() else text
    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]
    l.sort(key=alphanum_key)
    return l


def normalize_image(img, maximum):
    """
    Normalizes all pixels to values between 0 and maximum.

    @param img: numpy array
    @param maximum: scalar value

    @return numpy array of same size as img
    """
    img /= img.max() / maximum
    return img


def save_image(img_array, path):
    """
    Saves an image from a three-dimensional numpy array under path.

    @param img_array: two-dimensional numpy array
    @param path: string
    """
    img = PILImage.fromarray(img_array.astype('uint8'))
    img = img.convert('RGB')
    img.save(path)


def get_image_array(path, image_size=None):
    """
    Reads image from file and transforms it to numpy array.

    @param path: string
    @param image_size: (optional) size of the image

    @param img_array: numpy array
    """
    if image_size == None:
        return np.asarray(PILImage.open(path)).astype(np.float32)
    return np.asarray(PILImage.open(path).resize(image_size)).astype(np.float32)


def get_generated_images(path, p=96):
    """
    Cuts generated images from image saved in network output format.

    @param path: path to image to cut generated images from.

    @return: numpy array of shape 49x96x96x3
    """
    img = get_image_array(path)
    res = []
    for r in range(7):
        for c in range(7):
            single_image = img[r * p:(r + 1) * p, p * (c + 3):p * (c + 4)]
            res.append(single_image)
    return np.asarray(res)


def tile_to_square(images):
    """
    Transforms numpy array of 49x96x96 to numpy array of 672x672 by tiling.

    @param images: numpy array of shape 49x96x96

    @return: numpy array of shape 672x672
    """
    # Build final image from components
    frame = np.zeros([96 * 7, 96 * 7])
    for index, image in enumerate(images):
        index_column = index % 7
        index_row = index // 7
        frame[(index_row * 96):((index_row + 1) * 96), (index_column * 96):((index_column + 1) * 96)] = image
    return frame


def save_generated_output(inp, generated_outp, path, valence, arousal):
    """
    Save the output generated by the network.
    """
    for i in range(len(generated_outp)):
        save_path, _ = path.split('.jp')
        if not os.path.exists(save_path):
            os.makedirs(save_path)

        save_image((generated_outp[i] + 1) * 255 / 2,
                   save_path + "/ar_" + str(arousal[i][0]) + "_val_" + str(valence[i][0]) + ".jpg")


def load_image_as_network_input(image_path, imageSize=(96, 96), normalise=True):
    """
    Load image and normalize it to pixel values in [-1,1].

    @param image_path: path to image (string)

    @return: numpy array of size 96x96x3
    """
    image = get_image_array(image_path, image_size=imageSize)
    if normalise:
        image = normalize_image(image, 2)
        return image - 1
    else:
        return image


def generate_images(loadPath, valence, arousal, sess, graph, valid_faces, path_to_dir='./logs/faces/', path_to_out_dir='./logs/faces_imagined/'):
    with graph.as_default():
        tf.compat.v1.keras.backend.set_session(sess)
        # restore graph
        # tf.compat.v1.train.export_meta_graph(loadPath + '/01_model.meta')
        new_saver = tf.compat.v1.train.import_meta_graph(loadPath + '01_model.meta')
        new_saver.restore(sess, tf.compat.v1.train.latest_checkpoint(loadPath))
        # graph = tf.compat.v1.get_default_graph()

        arousal_tensor = graph.get_tensor_by_name("arousal_labels:0")
        valence_tensor = graph.get_tensor_by_name("valence_labels:0")
        images_tensor = graph.get_tensor_by_name("input_images:0")

        # randomly selecting images
        # files = os.listdir(path_to_dir) # [-10:]
        files = valid_faces

        # load input
        files_already = os.listdir(path_to_out_dir)
        files = [f.split("/")[-1] for f in files if not f in files_already]

        for file in files:
            i = load_image_as_network_input(path_to_dir + "/" + file).reshape((1, 96, 96, 3))
            query_images = np.tile(i, (49, 1, 1, 1))

            # create input for net
            feed_dict = {arousal_tensor: arousal, valence_tensor: valence, images_tensor: query_images}
            op_to_restore = sess.graph.get_tensor_by_name("generator/Tanh:0")
            # run
            x = sess.run(op_to_restore, feed_dict)
            # save
            save_generated_output(i, x, path_to_out_dir + "/" + file, valence=valence, arousal=arousal)


def preprocess(image, imageSize=(64, 64)):
    image = numpy.array(image)
    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    image = numpy.array(cv2.resize(image, imageSize))
    image = numpy.expand_dims(image, axis=0)
    image = image.astype('float32')
    image /= 255

    return image


def generate_encodings_facechannel(path_to_dir, sess, graph, model, mode='imagine'):
    '''
        dataX: Imagine,
        dataY: [arousal, valence]
    '''
    import keras.backend.tensorflow_backend as tb
    facechannel_faceSize = (64, 64)
    tb._SYMBOLIC_SCOPE.value = True

    dataX = []
    dataY_arousal = []
    dataY_valence = []

    with graph.as_default():
        tf.compat.v1.keras.backend.set_session(sess)
        # load input
        # modelDimensional = modelLoader.modelLoader(modelDictionary.DimensionalModel, printSummary=False)
        # ! IMAGINE MODE.
        if mode == 'imagine':
            files = sort_nicely(os.listdir(path_to_dir))

            # dense_model = modelDimensional.get_dense_model()

            for file in files:
                encodings = []
                arousal = []
                valence = []
                imgs = os.listdir(path_to_dir + file)
                processed_faces = []

                for img in imgs:
                    name = img.split(".jpg")[0]
                    _, ar, _, val = name.split("_")
                    arousal.append(float(ar))
                    valence.append(float(val))
                    processedFace = preprocess(load_image_as_network_input(path_to_dir + file + "/" + img,
                                                                           facechannel_faceSize,
                                                                           normalise=False).reshape((64, 64, 3)),
                                               facechannel_faceSize)
                    # get encodings
                    processed_faces.append(processedFace)
                arousal = np.array(arousal).reshape((49, 1))
                valence = np.array(valence).reshape((49, 1))
                encodings = np.array(
                    model.dense_model.predict(np.array(processed_faces), batch_size=model.BATCH_SIZE)).reshape(
                    (49, 200))

            if dataX == []:
                dataX = encodings
                dataY_arousal = arousal
                dataY_valence = valence
            else:
                dataX = np.vstack([dataX, encodings])
                dataY_arousal = np.vstack([dataY_arousal, arousal])
                dataY_valence = np.vstack([dataY_valence, valence])

            return dataX, np.hstack([dataY_arousal, dataY_valence])
        # ! ENCODE MODE.
        elif mode == 'encode':
            files = sort_nicely(os.listdir(path_to_dir))
            if len(files) < 90:
                to_fill = 90 - len(files)
                files = numpy.concatenate([files, [files[i] for i in numpy.random.randint(0, len(files), to_fill)]])
            # *_frame_timestamp.jpg -> timestamp
            face_time_stamps = [facename.split("_frame_")[1].split('.jpg')[0] for facename in files]
            processed_faces = []
            for file in files:
                processedFace = preprocess(load_image_as_network_input(path_to_dir + file,
                                                                       facechannel_faceSize,
                                                                       normalise=False).reshape((64, 64, 3)),
                                           facechannel_faceSize)
                # get encodings
                processed_faces.append(processedFace)

            # dense_model = modelDimensional.get_dense_model()
            encodings = model.dense_model.predict(np.array(processed_faces))
            encodings = np.array(encodings).reshape((len(encodings), 200))
            return encodings, face_time_stamps
        # ! INPUT MODE.
        elif mode == 'input':
            processed_faces = []

            files = sort_nicely(os.listdir(path_to_dir))

            if len(files) < 90:
                to_fill = 90 - len(files)
                files = numpy.concatenate([files, [files[i] for i in numpy.random.randint(0, len(files), to_fill)]])

            for file in files:
                processedFace = preprocess(
                    load_image_as_network_input(path_to_dir + file, facechannel_faceSize,
                                                normalise=False).reshape((64, 64, 3)),
                    facechannel_faceSize)
                # get encodings
                processed_faces.append(processedFace)

            dimensions = model.model.predict(np.array(processed_faces), batch_size=model.BATCH_SIZE)

            arousal = np.array([dimension for dimension in dimensions[0]]).reshape((len(dimensions[0]), 1))
            valence = np.array([dimension for dimension in dimensions[1]]).reshape((len(dimensions[1]), 1))
            encodings = model.dense_model.predict(np.array(processed_faces), batch_size=model.BATCH_SIZE)

            encodings = np.array(encodings).reshape((len(encodings), 200))

            return encodings, np.hstack([arousal, valence])


def generate_encodings(loadPath, path_to_out_dir, sess, graph, test=False):
    dataX = []
    dataY_arousal = []
    dataY_valence = []
    with sess:
        # restore graph
        new_saver = tf.compat.v1.train.import_meta_graph(loadPath + '/01_model.meta')
        new_saver.restore(sess, tf.compat.v1.train.latest_checkpoint(loadPath))
        # graph = tf.compat.v1.get_default_graph()

        arousal_tensor = graph.get_tensor_by_name("arousal_labels:0")
        valence_tensor = graph.get_tensor_by_name("valence_labels:0")
        images_tensor = graph.get_tensor_by_name("input_images:0")

        # load input
        files = sort_nicely(os.listdir(path_to_out_dir))[0:49]
        face_time_stamps = [facename.split("_frame_")[1].split('.jpg')[0] for facename in files]

        if not test:
            for file in files:
                arousal = []
                valence = []
                query_images = []
                imgs = os.listdir(path_to_out_dir + file)
                for img in imgs:
                    name = img.split(".jpg")[0]
                    _, ar, _, val = name.split("_")
                    arousal.append(float(ar))
                    valence.append(float(val))
                    query_images.append(
                        load_image_as_network_input(path_to_out_dir + file + "/" + img).reshape((1, 96, 96, 3)))
                arousal = np.array(arousal).reshape((49, 1))
                valence = np.array(valence).reshape((49, 1))
                query_images = np.array(query_images).reshape((49, 96, 96, 3))

                # create input for net
                feed_dict = {arousal_tensor: arousal, valence_tensor: valence, images_tensor: query_images}
                # Extract Encoder output
                encoder_out = sess.graph.get_tensor_by_name("encoder/Tanh:0")
                # run
                encodings = sess.run(encoder_out, feed_dict)

                if dataX == []:
                    dataX = encodings
                    dataY_arousal = arousal
                    dataY_valence = valence
                else:
                    dataX = np.vstack([dataX, encodings])
                    dataY_arousal = np.vstack([dataY_arousal, arousal])
                    dataY_valence = np.vstack([dataY_valence, valence])
            return dataX, np.hstack([dataY_arousal, dataY_valence])
        else:
            if len(files) < 49:
                to_fill = 49 - len(files)
                files = numpy.concatenate([files, [files[i] for i in numpy.random.randint(0, len(files), to_fill)]])
            query_images = []
            for file in files:
                query_images.append(load_image_as_network_input(path_to_out_dir + file).reshape((1, 96, 96, 3)))
            query_images = np.array(query_images).reshape((len(query_images), 96, 96, 3))

            # create input for net
            feed_dict = {images_tensor: query_images}
            encoder_out = sess.graph.get_tensor_by_name("encoder/Tanh:0")
            encodings = sess.run(encoder_out, feed_dict)
            if dataX == []:
                dataX = encodings
            else:
                dataX = np.vstack([dataX, encodings])
            return dataX, face_time_stamps


def apply_network_to_images_of_dir(loadPath, sess, graph, face_channel, valid_faces: list, path_to_dir='./logs/faces/', path_to_out_dir='./logs/faces_imagined/'):
    """
    Applies the trained network to all images found in path_to_dir for 49 emotions respectively.
    Saves the output in path_to_out_dir.

    @param path_to_dir: path to existing directory (string)
    @param path_to_out_dir: path to existing directory (string)
    """

    # valence
    valence = np.arange(0.75, -0.751, -0.25)
    valence = np.repeat(valence, 7).reshape((49, 1))
    # arousal
    arousal = [np.arange(0.75, -0.751, -0.25)]
    arousal = np.repeat(arousal, 7, axis=0)
    arousal = np.asarray([item for sublist in arousal for item in sublist]).reshape((49, 1))

    generate_images(loadPath, valence, arousal, sess, graph, valid_faces, path_to_dir, path_to_out_dir)

    return generate_encodings_facechannel(path_to_dir=path_to_out_dir, model=face_channel, mode='imagine', sess=sess, graph=graph)


def select_frames_to_imagine(path_to_dir, path_to_out_dir, size):
    files = np.random.choice(os.listdir(path_to_dir), size, replace=True)
    for file in files:
        shutil.copyfile(path_to_dir + file, path_to_out_dir + file)


def train_GDM(data, labels, trained_GWRs):
    '''
    Episodic-GWR supports multi-class neurons.
    Set the number of label classes per neuron and possible labels per class
    e.g. e_labels = [50, 10]
    is two labels per neuron, one with 50 and the other with 10 classes.
    Setting the n. of classes is done for experimental control but it is not
    necessary for associative GWR learning.
    '''

    # Training Data
    ds_vectors = data
    # Training Labels
    ds_labels = labels
    # ! HYPERPARAMETERS.
    # Number of context descriptors; Set to zero for frame-based evaluations.
    num_context = 0
    a_threshold = [0.6, 0.5]
    h_thresholds = [0.5, 0.5]
    beta = 0.7
    e_learning_rates = [0.087, 0.032]
    s_learning_rates = [0.087, 0.032]
    context = True

    # Initialising Episodic and Semantic Memory GWR Models.
    if trained_GWRs[0] is None:
        # Initialisint Episodic Memory
        g_episodic = EpisodicGWR()
        # Higher Max-nodes and lower age allow for faster learning with pattern-separated representations.
        g_episodic.init_network(data, ds_labels, num_context, max_nodes=len(data), age=600)
        # Initialising Semantic Memory
        # g_semantic = EpisodicGWR()
        # Lower Max-nodes and higher age allow for slower learning with pattern-complete representations.
        # g_semantic.init_network(data, ds_labels, num_context, max_nodes=len(data) // 2, age=1200)
    else:
        # Loading trained models for subsequent training runs.
        g_episodic, g_semantic = trained_GWRs

    """ Incremental training hyper-parameters """

    # Epochs per sample for incremental learning
    epochs = 5
    # Number of samples per epoch
    batch_size = 30

    """##############################################################################################"""
    """ ############################   Running Training of Memories  ################################"""
    """##############################################################################################"""

    for s in range(0, ds_vectors.shape[0], batch_size):
        # lm.write("Training Episodic Regular")
        g_episodic.train_egwr(ds_vectors[s: (s + batch_size)],
                              ds_labels[s: (s + batch_size)],
                              epochs,
                              a_threshold[0],
                              beta,
                              e_learning_rates,
                              context,
                              hab_threshold=h_thresholds[0],
                              regulated=0)

        """
        e_weights, e_labels = g_episodic.test_av(ds_vectors[s: (s + batch_size)],
                                                 ds_labels[s: (s + batch_size)],
                                                 test_accuracy=False)
        """
        '''
        g_semantic.train_egwr(e_weights,
                              ds_labels[s: (s + batch_size)],
                              epochs, a_threshold[1],
                              beta,
                              s_learning_rates,
                              context=False,
                              hab_threshold=h_thresholds[1],
                              regulated=1)
        '''

    # Evaluation with only Current Data
    # e_weights, e_labels = g_episodic.test_av(ds_vectors, ds_labels, test_accuracy=False)
    # s_weights, s_labels = g_semantic.test_av(e_weights, ds_labels, test_accuracy=False)

    return (g_episodic, g_episodic)


def annotate_GDM(trained_GWRs, test_data):
    g_episodic, g_semantic = trained_GWRs

    # Annotations from Episodic
    e_arousals, e_valences = g_episodic.annotate(test_data)

    # Annotations from Semantic
    # s_arousals, s_valences = g_semantic.annotate(test_data)

    return numpy.array([e_arousals, e_valences]).reshape((len(e_arousals)), 2), \
           numpy.array([e_arousals, e_valences]).reshape((len(e_arousals)), 2)


def unison_shuffled_copies(a, b):
    assert len(a) == len(b)
    p = numpy.random.permutation(len(a))
    return a[p], b[p]


def get_arousal_valence(log_dir: str, gwrs_path: str, graph, sess, face_channel, valid_faces: list, use_imagined: bool = True, model: EpisodicGWR = None):
    output_trained_GWRs = None

    if not os.path.isdir(log_dir + 'imagined'):
        os.mkdir(log_dir + 'imagined')

    input_trained_GWRs = (model, model)

    # Training with Imagined Data Every Interaction

    # Imagining Faces
    if use_imagined:
        imagined_data, imagined_labels = apply_network_to_images_of_dir(loadPath=CAAE_LoadPath, sess=sess, graph=graph, face_channel=face_channel,
                                                                        path_to_dir=log_dir + 'faces/',
                                                                        path_to_out_dir=log_dir + 'imagined/', valid_faces=valid_faces)

        print("Number of Imagined:", len(imagined_data))
    # Encode Input data
    input_data, input_labels = generate_encodings_facechannel(path_to_dir=log_dir + 'faces/', sess=sess, graph=graph,
                                                              model=face_channel, mode='input')

    if use_imagined:
        training_data = numpy.vstack([imagined_data, input_data])
        training_labels = numpy.vstack([imagined_labels, input_labels])
    else:
        training_data = np.array(input_data)
        training_labels = np.array(input_labels)
        
    print("Input Data Shape:", input_data.shape)
    print("Imagined Data Shape:", imagined_data.shape)
    print("Training Data Shape:", training_data.shape)


    output_trained_GWRs = train_GDM(data=training_data,
                                    labels=training_labels,
                                    trained_GWRs=input_trained_GWRs)

    '''
    encodings, face_time_stamps = generate_encodings_facechannel(path_to_dir=log_dir + 'faces/', sess=sess, graph=graph,
                                                                 mode="encode")

    episodic_results, semantic_results = annotate_GDM(output_trained_GWRs, encodings)

    arousal_valence_list = []

    for r in range(len(face_time_stamps)):
        arousal_valence_list.append(numpy.array(episodic_results[r]).reshape((2, 1, 1)))
    '''
    if not os.path.exists(gwrs_path):
        os.makedirs(gwrs_path)
    gtls.export_network(gwrs_path + "/GDM_E", output_trained_GWRs[0])
    gtls.export_network(gwrs_path + "/GDM_S", output_trained_GWRs[1])

    return output_trained_GWRs[0]

def predict(log_dir: str, model: EpisodicGWR, face_channel, graph, sess):
    encodings, face_time_stamps = generate_encodings_facechannel(path_to_dir=log_dir + 'faces/', sess=sess, graph=graph,
                                                                 model=face_channel, mode="encode")

    episodic_results, semantic_results = annotate_GDM([model, model], encodings)

    arousal_valence_list = []

    for r in range(len(face_time_stamps)):
        arousal_valence_list.append(numpy.array(episodic_results[r]).reshape((2, 1, 1)))

    return arousal_valence_list
